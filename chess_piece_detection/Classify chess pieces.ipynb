{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "J9Je0gsCod31",
    "outputId": "8833dd6e-a3e5-4069-d317-ef5357e87427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "## Below code required to access Google drive resources\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOORH4OBo7eT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras and TF imports\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6oYjHJlHpRk1"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn and Numpy imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VzI16CRFpmG_"
   },
   "outputs": [],
   "source": [
    "# set the location of the training and test images (change as required)\n",
    "location_of_train_data = \"../../Chess ID Public Data/output_train\"\n",
    "location_of_test_data = \"../../Chess ID Public Data/output_test\"\n",
    "model_folder_name = \"../models/15_12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bQvSOos5sFxt"
   },
   "outputs": [],
   "source": [
    "class_names = [\"bishop\", \"king\", \"knight\", \"pawn\", \"queen\", \"rook\", \"empty\"]\n",
    "class_names_reverse_mappings = {\"bishop\": 0, \"king\": 1, \"knight\":2, \"pawn\":3, \"queen\":4, \"rook\":5, \"empty\":6}\n",
    "class_names_folder_mappings = {\"bishop\": [\"bb\", \"wb\"], \"king\": [\"bk\", \"wk\"], \"knight\":[\"bn\", \"wn\"], \"pawn\":[\"bp\", \"wp\"], \"queen\":[\"bq\", \"wq\"], \"rook\":[\"br\", \"wr\"], \"empty\":[\"empty\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y60PmbgeqnfW"
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def get_required_data_with_labels_for_model(base_location, num_samples = None, dimensions=(299, 299)):\n",
    "  X, y = [], []\n",
    "  for class_name in class_names_folder_mappings:\n",
    "    for folder_name in class_names_folder_mappings[class_name]:\n",
    "      complete_path = os.path.join(base_location, folder_name)\n",
    "      print(\"Reading the files from the location {0}\".format(complete_path))\n",
    "      current_samples = 0\n",
    "      for image_file_name in os.listdir(complete_path):\n",
    "        \n",
    "        # check if the current file is an image file with jpg extension\n",
    "        if image_file_name.endswith(\".jpg\"):\n",
    "          current_samples += 1\n",
    "          img_path = os.path.join(complete_path, image_file_name)\n",
    "          \n",
    "          # basic pre-processing of the images\n",
    "          img = image.load_img(img_path, target_size=dimensions)\n",
    "          x = image.img_to_array(img)\n",
    "          x = preprocess_input(x)\n",
    "          \n",
    "          X.append(x)\n",
    "          class_name_id = class_names_reverse_mappings[class_name]\n",
    "          y.append(class_name_id)\n",
    "        \n",
    "          if ((num_samples is not None) and (current_samples == num_samples)):\n",
    "            break\n",
    "  \n",
    "  return X, y    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "v0pQBq7JxJNz",
    "outputId": "b65cfba0-3156-4c84-eaa7-8634786e77a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the files from the location ../../Chess ID Public Data/output_train\\bb\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wb\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\bk\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wk\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\bn\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wn\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\bp\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wp\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\bq\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wq\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\br\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\wr\n",
      "Reading the files from the location ../../Chess ID Public Data/output_train\\empty\n",
      "10360\n",
      "10360\n",
      "(299, 299, 3)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X, y = get_required_data_with_labels_for_model(location_of_train_data)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "print(X[0].shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGk0wJqayDc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9324, 299, 299, 3)\n",
      "(1036, 299, 299, 3)\n",
      "9324\n",
      "1036\n"
     ]
    }
   ],
   "source": [
    "# X_test, y_test = get_required_data_with_labels_for_model(location_of_test_data)\n",
    "X = np.array(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYSfIyAjzMWx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 3 6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 3 96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 3 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 6 192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 6 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 9 55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 4 144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 4 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 6 76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 9 82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 6 192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 9 288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 6 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 9 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 6 192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 9 55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 4 144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 4 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 6 76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 9 82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 6 192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 9 288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 6 192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 6 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 9 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 6 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 6 192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 6 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 9 55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 9 82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 3 1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 9 288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 1 114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 1 114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 1 384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 1 384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 1 172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 1 172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 1 576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 1 576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 1 576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 1 258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 1 576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 1 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 3 552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 1 331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 3 960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 1 576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 3 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 4 1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 3 1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 3 1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 3 1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 3 960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 3 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 4 1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, None, 4 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 3 1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 3 1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 3 1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 3 960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, None, 3 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, None, 3 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 1 576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            7175        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,908,135\n",
      "Trainable params: 2,105,351\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## update the base inception v3 model\n",
    "\n",
    "num_output_classes = len(class_names)\n",
    "\n",
    "# create the base pre-trained model\n",
    "inception_v3_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = inception_v3_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(num_output_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=inception_v3_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in inception_v3_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "opt = RMSprop(lr=0.00001)\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_folder_name):\n",
    "    os.makedirs(model_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3bjewU2s2SnH"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=os.path.join(model_folder_name, \"chess_pieces_inceptionv3_p1.hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=100, min_delta= 0.0001)\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zYzNRVKP3I7P"
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrfa8W7P3B3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9324 samples, validate on 1036 samples\n",
      "Epoch 1/500\n",
      "9324/9324 [==============================] - 41s 4ms/step - loss: 1.5518 - acc: 0.5446 - val_loss: 1.5538 - val_acc: 0.5975\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59749, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 2/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 1.2065 - acc: 0.6229 - val_loss: 1.4070 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59749 to 0.60618, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 3/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 1.0056 - acc: 0.6608 - val_loss: 1.3422 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60618\n",
      "Epoch 4/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.8641 - acc: 0.7177 - val_loss: 1.2834 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60618\n",
      "Epoch 5/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.7614 - acc: 0.7713 - val_loss: 1.2384 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.60618 to 0.60714, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 6/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.6783 - acc: 0.8093 - val_loss: 1.2779 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.60714 to 0.61100, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 7/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.6123 - acc: 0.8384 - val_loss: 1.3061 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.61100\n",
      "Epoch 8/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.5582 - acc: 0.8611 - val_loss: 1.3049 - val_acc: 0.6139\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.61100 to 0.61390, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 9/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.5151 - acc: 0.8749 - val_loss: 1.3868 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61390\n",
      "Epoch 10/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.4777 - acc: 0.8866 - val_loss: 1.3475 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.61390 to 0.62259, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 11/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.4435 - acc: 0.8952 - val_loss: 1.3135 - val_acc: 0.6332\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.62259 to 0.63320, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 12/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.4129 - acc: 0.9037 - val_loss: 1.3830 - val_acc: 0.6293\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.63320\n",
      "Epoch 13/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.3907 - acc: 0.9079 - val_loss: 1.4355 - val_acc: 0.6332\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.63320 to 0.63320, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 14/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.3670 - acc: 0.9157 - val_loss: 1.5092 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.63320\n",
      "Epoch 15/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.3482 - acc: 0.9195 - val_loss: 1.4418 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.63320 to 0.63514, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 16/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.3314 - acc: 0.9234 - val_loss: 1.5043 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.63514 to 0.63803, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 17/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.3138 - acc: 0.9269 - val_loss: 1.5607 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.63803\n",
      "Epoch 18/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.3004 - acc: 0.9304 - val_loss: 1.5728 - val_acc: 0.6371\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.63803\n",
      "Epoch 19/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2872 - acc: 0.9348 - val_loss: 1.5962 - val_acc: 0.6371\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.63803\n",
      "Epoch 20/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2777 - acc: 0.9355 - val_loss: 1.5728 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.63803 to 0.63996, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 21/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.2667 - acc: 0.9373 - val_loss: 1.6302 - val_acc: 0.6390\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.63996\n",
      "Epoch 22/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.2581 - acc: 0.9399 - val_loss: 1.6165 - val_acc: 0.6409\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.63996 to 0.64093, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 23/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.2467 - acc: 0.9434 - val_loss: 1.8469 - val_acc: 0.6293\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.64093\n",
      "Epoch 24/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2381 - acc: 0.9420 - val_loss: 1.6478 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.64093 to 0.64479, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 25/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2322 - acc: 0.9458 - val_loss: 1.7134 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.64479\n",
      "Epoch 26/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2252 - acc: 0.9454 - val_loss: 1.6419 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.64479 to 0.64865, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 27/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.2187 - acc: 0.9477 - val_loss: 1.7044 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.64865\n",
      "Epoch 28/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2123 - acc: 0.9488 - val_loss: 1.8596 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.64865\n",
      "Epoch 29/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.2058 - acc: 0.9509 - val_loss: 1.7637 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.64865\n",
      "Epoch 30/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1992 - acc: 0.9527 - val_loss: 1.7978 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.64865\n",
      "Epoch 31/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1951 - acc: 0.9547 - val_loss: 1.7522 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.64865 to 0.64961, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 32/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1906 - acc: 0.9542 - val_loss: 1.9964 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.64961\n",
      "Epoch 33/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1874 - acc: 0.9570 - val_loss: 1.8488 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.64961\n",
      "Epoch 34/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1825 - acc: 0.9561 - val_loss: 1.8410 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.64961\n",
      "Epoch 35/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1792 - acc: 0.9569 - val_loss: 1.9113 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.64961\n",
      "Epoch 36/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1741 - acc: 0.9567 - val_loss: 1.8364 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.64961\n",
      "Epoch 37/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1705 - acc: 0.9597 - val_loss: 1.8299 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.64961\n",
      "Epoch 38/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1675 - acc: 0.9594 - val_loss: 1.9727 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.64961\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1622 - acc: 0.9626 - val_loss: 2.0208 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.64961\n",
      "Epoch 40/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1618 - acc: 0.9635 - val_loss: 1.9109 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.64961\n",
      "Epoch 41/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1589 - acc: 0.9626 - val_loss: 1.9489 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.64961\n",
      "Epoch 42/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1533 - acc: 0.9651 - val_loss: 1.8802 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.64961 to 0.65058, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 43/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1522 - acc: 0.9648 - val_loss: 1.9260 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.65058\n",
      "Epoch 44/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1493 - acc: 0.9650 - val_loss: 2.0647 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.65058\n",
      "Epoch 45/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1459 - acc: 0.9654 - val_loss: 2.1289 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.65058\n",
      "Epoch 46/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1436 - acc: 0.9665 - val_loss: 2.0570 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.65058\n",
      "Epoch 47/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1391 - acc: 0.9671 - val_loss: 2.1107 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.65058\n",
      "Epoch 48/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1383 - acc: 0.9698 - val_loss: 2.0677 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.65058\n",
      "Epoch 49/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1345 - acc: 0.9701 - val_loss: 2.0273 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.65058\n",
      "Epoch 50/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1338 - acc: 0.9702 - val_loss: 2.1330 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.65058\n",
      "Epoch 51/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1326 - acc: 0.9689 - val_loss: 2.2325 - val_acc: 0.6409\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.65058\n",
      "Epoch 52/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1277 - acc: 0.9714 - val_loss: 2.2345 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.65058\n",
      "Epoch 53/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1258 - acc: 0.9706 - val_loss: 2.0034 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.65058\n",
      "Epoch 54/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1260 - acc: 0.9733 - val_loss: 2.1478 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.65058\n",
      "Epoch 55/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1219 - acc: 0.9730 - val_loss: 2.0262 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.65058\n",
      "Epoch 56/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1213 - acc: 0.9735 - val_loss: 2.1931 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.65058\n",
      "Epoch 57/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1190 - acc: 0.9717 - val_loss: 2.1715 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.65058\n",
      "Epoch 58/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1177 - acc: 0.9739 - val_loss: 2.2597 - val_acc: 0.6409\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.65058\n",
      "Epoch 59/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1160 - acc: 0.9739 - val_loss: 2.1331 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.65058\n",
      "Epoch 60/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1150 - acc: 0.9748 - val_loss: 2.0959 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.65058 to 0.65251, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 61/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1131 - acc: 0.9759 - val_loss: 2.2621 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.65251\n",
      "Epoch 62/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1114 - acc: 0.9755 - val_loss: 2.1767 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.65251\n",
      "Epoch 63/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1103 - acc: 0.9763 - val_loss: 2.2574 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.65251\n",
      "Epoch 64/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1079 - acc: 0.9767 - val_loss: 2.1633 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.65251\n",
      "Epoch 65/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.1069 - acc: 0.9770 - val_loss: 2.2370 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.65251\n",
      "Epoch 66/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1075 - acc: 0.9768 - val_loss: 2.1307 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.65251\n",
      "Epoch 67/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1061 - acc: 0.9765 - val_loss: 2.2325 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.65251\n",
      "Epoch 68/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1028 - acc: 0.9783 - val_loss: 2.1289 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.65251\n",
      "Epoch 69/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.1010 - acc: 0.9795 - val_loss: 2.2116 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.65251\n",
      "Epoch 70/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0990 - acc: 0.9799 - val_loss: 2.2890 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.65251\n",
      "Epoch 71/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0989 - acc: 0.9793 - val_loss: 2.2945 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.65251\n",
      "Epoch 72/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0947 - acc: 0.9811 - val_loss: 2.3691 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.65251\n",
      "Epoch 73/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0954 - acc: 0.9805 - val_loss: 2.2594 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.65251\n",
      "Epoch 74/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0947 - acc: 0.9808 - val_loss: 2.3065 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.65251\n",
      "Epoch 75/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0941 - acc: 0.9797 - val_loss: 2.2488 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.65251\n",
      "Epoch 76/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0928 - acc: 0.9805 - val_loss: 2.2479 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.65251\n",
      "Epoch 77/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0922 - acc: 0.9798 - val_loss: 2.2703 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.65251\n",
      "Epoch 78/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0899 - acc: 0.9811 - val_loss: 2.4264 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.65251\n",
      "Epoch 79/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0889 - acc: 0.9821 - val_loss: 2.3567 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.65251\n",
      "Epoch 80/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0881 - acc: 0.9816 - val_loss: 2.4120 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.65251\n",
      "Epoch 81/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0881 - acc: 0.9828 - val_loss: 2.4052 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.65251\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0858 - acc: 0.9826 - val_loss: 2.2735 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.65251\n",
      "Epoch 83/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0844 - acc: 0.9834 - val_loss: 2.4504 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.65251\n",
      "Epoch 84/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0845 - acc: 0.9829 - val_loss: 2.4055 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.65251\n",
      "Epoch 85/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0828 - acc: 0.9829 - val_loss: 2.3828 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.65251\n",
      "Epoch 86/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0832 - acc: 0.9826 - val_loss: 2.4935 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.65251\n",
      "Epoch 87/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0806 - acc: 0.9853 - val_loss: 2.3325 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.65251\n",
      "Epoch 88/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0805 - acc: 0.9838 - val_loss: 2.3705 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.65251\n",
      "Epoch 89/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0804 - acc: 0.9835 - val_loss: 2.4081 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.65251\n",
      "Epoch 90/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0794 - acc: 0.9850 - val_loss: 2.2569 - val_acc: 0.6544\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.65251 to 0.65444, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 91/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0783 - acc: 0.9851 - val_loss: 2.4377 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.65444\n",
      "Epoch 92/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0773 - acc: 0.9852 - val_loss: 2.3997 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.65444\n",
      "Epoch 93/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0770 - acc: 0.9839 - val_loss: 2.4838 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.65444\n",
      "Epoch 94/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0734 - acc: 0.9867 - val_loss: 2.3783 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.65444\n",
      "Epoch 95/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0736 - acc: 0.9876 - val_loss: 2.3658 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.65444\n",
      "Epoch 96/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0739 - acc: 0.9843 - val_loss: 2.3271 - val_acc: 0.6573\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.65444 to 0.65734, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 97/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0735 - acc: 0.9858 - val_loss: 2.5189 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.65734\n",
      "Epoch 98/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0730 - acc: 0.9855 - val_loss: 2.4238 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.65734\n",
      "Epoch 99/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0703 - acc: 0.9877 - val_loss: 2.3415 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.65734\n",
      "Epoch 100/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0705 - acc: 0.9868 - val_loss: 2.5543 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.65734\n",
      "Epoch 101/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0705 - acc: 0.9876 - val_loss: 2.5494 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.65734\n",
      "Epoch 102/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0700 - acc: 0.9881 - val_loss: 2.4013 - val_acc: 0.6544\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.65734\n",
      "Epoch 103/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0693 - acc: 0.9870 - val_loss: 2.4276 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.65734\n",
      "Epoch 104/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0673 - acc: 0.9887 - val_loss: 2.4314 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.65734\n",
      "Epoch 105/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0684 - acc: 0.9867 - val_loss: 2.4507 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.65734\n",
      "Epoch 106/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0648 - acc: 0.9885 - val_loss: 2.4979 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.65734\n",
      "Epoch 107/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0638 - acc: 0.9880 - val_loss: 2.4966 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.65734\n",
      "Epoch 108/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0661 - acc: 0.9878 - val_loss: 2.4865 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.65734\n",
      "Epoch 109/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0665 - acc: 0.9868 - val_loss: 2.4525 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.65734\n",
      "Epoch 110/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0638 - acc: 0.9887 - val_loss: 2.3437 - val_acc: 0.6573\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.65734\n",
      "Epoch 111/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0644 - acc: 0.9878 - val_loss: 2.4329 - val_acc: 0.6535\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.65734\n",
      "Epoch 112/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0610 - acc: 0.9906 - val_loss: 2.5259 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.65734\n",
      "Epoch 113/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0637 - acc: 0.9873 - val_loss: 2.5622 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.65734\n",
      "Epoch 114/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0603 - acc: 0.9896 - val_loss: 2.5768 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.65734\n",
      "Epoch 115/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0594 - acc: 0.9906 - val_loss: 2.5191 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.65734\n",
      "Epoch 116/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0624 - acc: 0.9881 - val_loss: 2.3601 - val_acc: 0.6612\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.65734 to 0.66120, saving model to ../models/15_12\\chess_pieces_inceptionv3_p1.hdf5\n",
      "Epoch 117/500\n",
      "9324/9324 [==============================] - 30s 3ms/step - loss: 0.0587 - acc: 0.9900 - val_loss: 2.5121 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.66120\n",
      "Epoch 118/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0582 - acc: 0.9900 - val_loss: 2.6827 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.66120\n",
      "Epoch 119/500\n",
      "9324/9324 [==============================] - 31s 3ms/step - loss: 0.0585 - acc: 0.9906 - val_loss: 2.6183 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.66120\n",
      "Epoch 120/500\n",
      "9200/9324 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9900"
     ]
    }
   ],
   "source": [
    "#X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)\n",
    "history = model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o98c0gGA3Wkd"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QX8HjeHx3ddt"
   },
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_folder_name, \"chess_pieces_inceptionv3_p1.hdf5\"))\n",
    "\n",
    "## Fine tune some inception layers\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "opt = RMSprop(lr=0.00001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rOKA0taT4WAH"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=os.path.join(model_folder_name, \"chess_pieces_inceptionv3_p2.hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=50, min_delta= 0.0001)\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kXGir3oX4aVy"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eivTGHd4i5V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9324 samples, validate on 1036 samples\n",
      "Epoch 1/100\n",
      "9324/9324 [==============================] - 43s 5ms/step - loss: 0.0556 - acc: 0.9863 - val_loss: 1.3112 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72780, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 2/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0233 - acc: 0.9968 - val_loss: 1.4285 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72780 to 0.73359, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 3/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0114 - acc: 0.9990 - val_loss: 1.4045 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.73359 to 0.73649, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 4/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0058 - acc: 0.9998 - val_loss: 1.5304 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.73649\n",
      "Epoch 5/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0035 - acc: 0.9996 - val_loss: 1.4171 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.73649 to 0.74517, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 6/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0018 - acc: 0.9999 - val_loss: 1.4194 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.74517 to 0.75386, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 7/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5340 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.75386\n",
      "Epoch 8/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 6.0518e-04 - acc: 0.9999 - val_loss: 1.3280 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.75386 to 0.76834, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 9/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 5.1568e-04 - acc: 1.0000 - val_loss: 1.2992 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.76834 to 0.77027, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 10/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 3.9823e-04 - acc: 1.0000 - val_loss: 1.3572 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77027\n",
      "Epoch 11/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.0140e-04 - acc: 0.9998 - val_loss: 1.5451 - val_acc: 0.7519\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77027\n",
      "Epoch 12/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5853e-04 - acc: 1.0000 - val_loss: 1.3585 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.77027 to 0.77317, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 13/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 6.7194e-05 - acc: 1.0000 - val_loss: 1.5028 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77317\n",
      "Epoch 14/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 5.6095e-05 - acc: 1.0000 - val_loss: 1.6694 - val_acc: 0.7529\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77317\n",
      "Epoch 15/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 3.4245e-05 - acc: 1.0000 - val_loss: 1.7568 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.77317\n",
      "Epoch 16/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.0590e-05 - acc: 1.0000 - val_loss: 1.8317 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.77317\n",
      "Epoch 17/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.4430e-05 - acc: 1.0000 - val_loss: 1.7947 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.77317\n",
      "Epoch 18/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.5694e-05 - acc: 1.0000 - val_loss: 1.7038 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77317\n",
      "Epoch 19/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.1008e-05 - acc: 1.0000 - val_loss: 1.8068 - val_acc: 0.7577\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77317\n",
      "Epoch 20/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.8752e-05 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77317\n",
      "Epoch 21/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.2662e-06 - acc: 1.0000 - val_loss: 1.7407 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77317\n",
      "Epoch 22/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 5.6845e-06 - acc: 1.0000 - val_loss: 1.8198 - val_acc: 0.7577\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77317\n",
      "Epoch 23/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.1464e-06 - acc: 1.0000 - val_loss: 1.7496 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77317\n",
      "Epoch 24/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.0924e-06 - acc: 1.0000 - val_loss: 1.6320 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.77317 to 0.77992, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 25/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.1999e-06 - acc: 1.0000 - val_loss: 1.7669 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77992\n",
      "Epoch 26/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 9.4353e-06 - acc: 1.0000 - val_loss: 1.7324 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.77992\n",
      "Epoch 27/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.6581e-05 - acc: 1.0000 - val_loss: 1.8429 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.77992\n",
      "Epoch 28/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.0082e-05 - acc: 1.0000 - val_loss: 1.8041 - val_acc: 0.7616\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.77992\n",
      "Epoch 29/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.9757e-06 - acc: 1.0000 - val_loss: 1.7334 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.77992\n",
      "Epoch 30/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.7517e-06 - acc: 1.0000 - val_loss: 1.7922 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.77992\n",
      "Epoch 31/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.7505e-06 - acc: 1.0000 - val_loss: 1.8030 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.77992\n",
      "Epoch 32/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5014e-06 - acc: 1.0000 - val_loss: 1.7913 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.77992\n",
      "Epoch 33/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 3.2670e-05 - acc: 1.0000 - val_loss: 1.7144 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.77992\n",
      "Epoch 34/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.4885e-06 - acc: 1.0000 - val_loss: 1.7823 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.77992\n",
      "Epoch 35/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.7093e-06 - acc: 1.0000 - val_loss: 1.9187 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.77992\n",
      "Epoch 36/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5568e-04 - acc: 0.9999 - val_loss: 1.9281 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.77992\n",
      "Epoch 37/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.1461e-06 - acc: 1.0000 - val_loss: 1.8169 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.77992\n",
      "Epoch 38/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.1561e-06 - acc: 1.0000 - val_loss: 1.7902 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.77992\n",
      "Epoch 39/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 6.5018e-06 - acc: 1.0000 - val_loss: 1.7478 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77992\n",
      "Epoch 40/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5871e-06 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77992\n",
      "Epoch 41/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.8024e-06 - acc: 1.0000 - val_loss: 1.8861 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77992\n",
      "Epoch 42/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 3.5889e-06 - acc: 1.0000 - val_loss: 1.9183 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77992\n",
      "Epoch 43/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.7164e-05 - acc: 1.0000 - val_loss: 1.7477 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77992\n",
      "Epoch 44/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0537e-06 - acc: 1.0000 - val_loss: 1.7824 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77992\n",
      "Epoch 45/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.4799e-06 - acc: 1.0000 - val_loss: 1.7089 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.77992 to 0.78475, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 46/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.3056e-06 - acc: 1.0000 - val_loss: 1.6647 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.78475\n",
      "Epoch 47/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.1879e-05 - acc: 1.0000 - val_loss: 1.8010 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.78475\n",
      "Epoch 48/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.6699e-05 - acc: 1.0000 - val_loss: 1.7512 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.78475\n",
      "Epoch 49/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.2060e-06 - acc: 1.0000 - val_loss: 1.7449 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.78475\n",
      "Epoch 50/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 5.7918e-06 - acc: 1.0000 - val_loss: 1.7187 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.78475\n",
      "Epoch 51/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5206e-06 - acc: 1.0000 - val_loss: 1.7078 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.78475 to 0.78571, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 52/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0351e-06 - acc: 1.0000 - val_loss: 1.6698 - val_acc: 0.7886\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.78571 to 0.78861, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 53/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.5528e-05 - acc: 1.0000 - val_loss: 1.6774 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.78861\n",
      "Epoch 54/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.9829e-07 - acc: 1.0000 - val_loss: 1.6917 - val_acc: 0.7886\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.78861\n",
      "Epoch 55/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.7106e-07 - acc: 1.0000 - val_loss: 1.6877 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.78861 to 0.78958, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 56/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.1469e-06 - acc: 1.0000 - val_loss: 1.8000 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.78958\n",
      "Epoch 57/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 5.3197e-06 - acc: 1.0000 - val_loss: 1.8349 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.78958\n",
      "Epoch 58/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.4398e-07 - acc: 1.0000 - val_loss: 1.8528 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.78958\n",
      "Epoch 59/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.2914e-06 - acc: 1.0000 - val_loss: 1.8727 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.78958\n",
      "Epoch 60/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 9.7629e-07 - acc: 1.0000 - val_loss: 1.8802 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.78958\n",
      "Epoch 61/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.4806e-06 - acc: 1.0000 - val_loss: 1.8145 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.78958\n",
      "Epoch 62/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.4601e-07 - acc: 1.0000 - val_loss: 1.7478 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.78958\n",
      "Epoch 63/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.4282e-06 - acc: 1.0000 - val_loss: 1.6493 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.78958 to 0.79247, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 64/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.5835e-07 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.79247\n",
      "Epoch 65/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.4776e-06 - acc: 1.0000 - val_loss: 1.9403 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.79247\n",
      "Epoch 66/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.5555e-06 - acc: 1.0000 - val_loss: 2.0074 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.79247\n",
      "Epoch 67/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.5017e-07 - acc: 1.0000 - val_loss: 1.9391 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.79247\n",
      "Epoch 68/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.2819e-07 - acc: 1.0000 - val_loss: 1.8590 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.79247\n",
      "Epoch 69/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.9839e-07 - acc: 1.0000 - val_loss: 1.8807 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.79247\n",
      "Epoch 70/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0458e-06 - acc: 1.0000 - val_loss: 1.9463 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.79247\n",
      "Epoch 71/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.6116e-07 - acc: 1.0000 - val_loss: 1.9128 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.79247\n",
      "Epoch 72/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.6521e-07 - acc: 1.0000 - val_loss: 1.8811 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.79247\n",
      "Epoch 73/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.4997e-06 - acc: 1.0000 - val_loss: 1.8742 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.79247\n",
      "Epoch 74/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.2702e-07 - acc: 1.0000 - val_loss: 1.9234 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.79247\n",
      "Epoch 75/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.4485e-05 - acc: 1.0000 - val_loss: 2.0654 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.79247\n",
      "Epoch 76/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.0966e-07 - acc: 1.0000 - val_loss: 1.9728 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.79247\n",
      "Epoch 77/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.8570e-07 - acc: 1.0000 - val_loss: 1.9542 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.79247\n",
      "Epoch 78/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 9.9543e-07 - acc: 1.0000 - val_loss: 1.8904 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.79247\n",
      "Epoch 79/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.4453e-06 - acc: 1.0000 - val_loss: 1.9307 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.79247\n",
      "Epoch 80/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.2473e-07 - acc: 1.0000 - val_loss: 1.9035 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.79247\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324/9324 [==============================] - 35s 4ms/step - loss: 9.2051e-07 - acc: 1.0000 - val_loss: 1.9892 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.79247\n",
      "Epoch 82/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.2805e-07 - acc: 1.0000 - val_loss: 1.8924 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.79247\n",
      "Epoch 83/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.6841e-07 - acc: 1.0000 - val_loss: 1.7600 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.79247\n",
      "Epoch 84/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.8405e-06 - acc: 1.0000 - val_loss: 1.7972 - val_acc: 0.7876\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.79247\n",
      "Epoch 85/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.3055e-07 - acc: 1.0000 - val_loss: 1.7037 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.79247 to 0.79730, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 86/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.4258e-07 - acc: 1.0000 - val_loss: 1.7470 - val_acc: 0.7934\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.79730\n",
      "Epoch 87/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.2244e-07 - acc: 1.0000 - val_loss: 1.7773 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.79730\n",
      "Epoch 88/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.0202e-07 - acc: 1.0000 - val_loss: 1.8267 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.79730\n",
      "Epoch 89/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 4.5845e-05 - acc: 1.0000 - val_loss: 1.7339 - val_acc: 0.7954\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.79730\n",
      "Epoch 90/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 6.5815e-06 - acc: 1.0000 - val_loss: 1.6897 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.79730\n",
      "Epoch 91/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.3919e-06 - acc: 1.0000 - val_loss: 1.6536 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00091: val_acc improved from 0.79730 to 0.80019, saving model to ../models/15_12\\chess_pieces_inceptionv3_p2.hdf5\n",
      "Epoch 92/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 8.2247e-07 - acc: 1.0000 - val_loss: 1.6911 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.80019\n",
      "Epoch 93/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0723e-06 - acc: 1.0000 - val_loss: 1.7248 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.80019\n",
      "Epoch 94/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.3443e-07 - acc: 1.0000 - val_loss: 1.7801 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.80019\n",
      "Epoch 95/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.3709e-06 - acc: 1.0000 - val_loss: 1.8196 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.80019\n",
      "Epoch 96/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0322e-06 - acc: 1.0000 - val_loss: 1.7562 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.80019\n",
      "Epoch 97/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 9.8070e-07 - acc: 1.0000 - val_loss: 1.6777 - val_acc: 0.7963\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.80019\n",
      "Epoch 98/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 2.4844e-06 - acc: 1.0000 - val_loss: 1.8010 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.80019\n",
      "Epoch 99/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 1.0800e-06 - acc: 1.0000 - val_loss: 1.6581 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.80019\n",
      "Epoch 100/100\n",
      "9324/9324 [==============================] - 35s 4ms/step - loss: 7.5464e-07 - acc: 1.0000 - val_loss: 1.7483 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.80019\n"
     ]
    }
   ],
   "source": [
    "history_2 = model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIHUJ9yO43-U"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AUAmKCek49Ws"
   },
   "outputs": [],
   "source": [
    "## Helper method to print a confusion matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rX-e3Cxt5GPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: [1.6535627887976216, 0.8001930501930502]\n",
      "Normalized confusion matrix\n",
      "[[0.26315789 0.01754386 0.         0.71052632 0.00877193 0.\n",
      "  0.        ]\n",
      " [0.01612903 0.85483871 0.         0.11290323 0.01612903 0.\n",
      "  0.        ]\n",
      " [0.00925926 0.         0.57407407 0.41666667 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.06557377 0.         0.2295082  0.70491803 0.\n",
      "  0.        ]\n",
      " [0.01694915 0.         0.         0.83050847 0.         0.15254237\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_weights(os.path.join(model_folder_name, \"chess_pieces_inceptionv3_p2.hdf5\"))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Score: \" + str(score))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "test_predictions = model.predict(X_test, batch_size=batch_size)\n",
    "y_test_pred = [np.argmax(x) for x in test_predictions]\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classify chess pieces.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
